{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8x8 maze object\n",
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function allows a visual representation of the maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizes the maze\n",
    "def show(qmaze):\n",
    "    # show grid lines\n",
    "    plt.grid('on')\n",
    "    \n",
    "    # rows and columns of maze\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    \n",
    "    # get current axes\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # add ticks for visual flair\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    # return array copy of maze\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    \n",
    "    # \n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "        \n",
    "    # current location of pirate cell\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3   # pirate cell shading\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # treasure cell shading\n",
    "    \n",
    "    # plot and show \n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four potential movements for the pirate cell\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "\n",
    "# exploration factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulates a full game, needs the trained model\n",
    "def play_game(model, qmaze, pirate_cell):\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check of unsolvable mazes\n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you have been given in this block will build the neural network model. Review the code and note the number of layers, as well as the activation, optimizer, and loss functions that are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "def build_model(maze):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning\n",
    "def qtrain(model, maze, **opt):\n",
    "\n",
    "    # exploration factor\n",
    "    global epsilon \n",
    "\n",
    "    # number of epochs\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "\n",
    "    # maximum memory to store episodes\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "\n",
    "    # maximum data size for training\n",
    "    data_size = opt.get('data_size', 50)\n",
    "\n",
    "    # start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = TreasureMaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    \n",
    "    #each iteration will evaluate an agent attempting the too solve the maze\n",
    "    for epoch in range(n_epoch):\n",
    "        #variables for tracking progress the agent makes in the game\n",
    "        loss = 0.0\n",
    "        game_over = False\n",
    "        n_episodes = 0\n",
    "        \n",
    "        #randomly select a free cell\n",
    "        #stating location\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        \n",
    "        #set agent at starting position\n",
    "        qmaze.reset(agent_cell)\n",
    "        \n",
    "        #gets current state\n",
    "        envstate = qmaze.observe()\n",
    "        \n",
    "        #keep looping until the agent gets to the target location or\n",
    "        #if the reward goes below the minimum reward:\n",
    "        while not game_over:\n",
    "            #get possible move locations\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            \n",
    "            #invalid starting location, end game \n",
    "            if not valid_actions:\n",
    "                break\n",
    "\n",
    "            #get previous envstate\n",
    "            prev_envstate = envstate \n",
    "            \n",
    "            #mutation, 10% of the time the agent will randomly explore instead of \"thinking ahead\"\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions) #Actions = randomly chosen action\n",
    "            else:\n",
    "                #grabs the first prediction after Sequential.predict() analysis the environment state\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "                \n",
    "                \n",
    "            #apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over=True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over=True\n",
    "            else:\n",
    "                game_over=False\n",
    "        \n",
    "            #store episode in Experience Replay Object\n",
    "            episode = [prev_envstate, action, reward, envstate, game_status]\n",
    "            n_episodes += 1\n",
    "            \n",
    "            #stores 100 recent episodes, for learning\n",
    "            experience.remember(episode)\n",
    "            \n",
    "            #train neural network model and evaluate loss\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            History = model.fit(\n",
    "                inputs, \n",
    "                targets, \n",
    "                epochs=20,\n",
    "                batch_size=50,\n",
    "                verbose=0\n",
    "            ) \n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "            \n",
    "        #if winrate is above threshold and model passes completion check\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "        \n",
    "\n",
    "        ###information aspect of the AI###\n",
    "        #Print the epoch, loss, episodes, win count, and win rate for each epoch\n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # We simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won.\n",
    "        if win_rate > 0.9 : epsilon = 0.05 #5% exploration chance\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #determine the total time for training\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29d4663ea90>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up teting environment for model\n",
    "qmaze = TreasureMaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Loss: 0.0009 | Episodes: 86 | Win count: 1 | Win rate: 0.000 | time: 184.5 seconds\n",
      "Epoch: 001/14999 | Loss: 0.0010 | Episodes: 122 | Win count: 2 | Win rate: 0.000 | time: 8.33 minutes\n",
      "Epoch: 002/14999 | Loss: 0.0009 | Episodes: 156 | Win count: 2 | Win rate: 0.000 | time: 14.97 minutes\n",
      "Epoch: 003/14999 | Loss: 0.0007 | Episodes: 3 | Win count: 3 | Win rate: 0.000 | time: 15.10 minutes\n",
      "Epoch: 004/14999 | Loss: 0.0011 | Episodes: 44 | Win count: 4 | Win rate: 0.000 | time: 16.97 minutes\n",
      "Epoch: 005/14999 | Loss: 0.0008 | Episodes: 36 | Win count: 5 | Win rate: 0.000 | time: 18.54 minutes\n",
      "Epoch: 006/14999 | Loss: 0.0007 | Episodes: 9 | Win count: 6 | Win rate: 0.000 | time: 18.93 minutes\n",
      "Epoch: 007/14999 | Loss: 0.0010 | Episodes: 10 | Win count: 7 | Win rate: 0.000 | time: 19.38 minutes\n",
      "Epoch: 008/14999 | Loss: 0.0005 | Episodes: 51 | Win count: 8 | Win rate: 0.000 | time: 21.62 minutes\n",
      "Epoch: 009/14999 | Loss: 0.0014 | Episodes: 30 | Win count: 9 | Win rate: 0.000 | time: 22.95 minutes\n",
      "Epoch: 010/14999 | Loss: 0.0011 | Episodes: 104 | Win count: 10 | Win rate: 0.000 | time: 27.13 minutes\n",
      "Epoch: 011/14999 | Loss: 0.0004 | Episodes: 30 | Win count: 11 | Win rate: 0.000 | time: 28.35 minutes\n",
      "Epoch: 012/14999 | Loss: 0.0011 | Episodes: 43 | Win count: 12 | Win rate: 0.000 | time: 30.28 minutes\n",
      "Epoch: 013/14999 | Loss: 0.0009 | Episodes: 35 | Win count: 13 | Win rate: 0.000 | time: 31.64 minutes\n",
      "Epoch: 014/14999 | Loss: 0.0007 | Episodes: 35 | Win count: 14 | Win rate: 0.000 | time: 33.00 minutes\n",
      "Epoch: 015/14999 | Loss: 0.0005 | Episodes: 13 | Win count: 15 | Win rate: 0.000 | time: 33.50 minutes\n",
      "Epoch: 016/14999 | Loss: 0.0008 | Episodes: 33 | Win count: 16 | Win rate: 0.000 | time: 34.80 minutes\n",
      "Epoch: 017/14999 | Loss: 0.0007 | Episodes: 13 | Win count: 17 | Win rate: 0.000 | time: 35.31 minutes\n",
      "Epoch: 018/14999 | Loss: 0.0011 | Episodes: 10 | Win count: 18 | Win rate: 0.000 | time: 35.70 minutes\n",
      "Epoch: 019/14999 | Loss: 0.0007 | Episodes: 9 | Win count: 19 | Win rate: 0.000 | time: 36.04 minutes\n",
      "Epoch: 020/14999 | Loss: 0.0012 | Episodes: 29 | Win count: 20 | Win rate: 0.000 | time: 37.17 minutes\n",
      "Epoch: 021/14999 | Loss: 0.0007 | Episodes: 16 | Win count: 21 | Win rate: 0.000 | time: 37.79 minutes\n",
      "Epoch: 022/14999 | Loss: 0.0008 | Episodes: 23 | Win count: 22 | Win rate: 0.000 | time: 38.68 minutes\n",
      "Epoch: 023/14999 | Loss: 0.0005 | Episodes: 66 | Win count: 23 | Win rate: 0.000 | time: 41.23 minutes\n",
      "Epoch: 024/14999 | Loss: 0.0010 | Episodes: 66 | Win count: 24 | Win rate: 0.000 | time: 43.78 minutes\n",
      "Epoch: 025/14999 | Loss: 0.0016 | Episodes: 41 | Win count: 25 | Win rate: 0.000 | time: 45.37 minutes\n",
      "Epoch: 026/14999 | Loss: 0.0011 | Episodes: 5 | Win count: 26 | Win rate: 0.000 | time: 45.57 minutes\n",
      "Epoch: 027/14999 | Loss: 0.0005 | Episodes: 137 | Win count: 27 | Win rate: 0.000 | time: 50.88 minutes\n",
      "Epoch: 028/14999 | Loss: 0.0006 | Episodes: 2 | Win count: 28 | Win rate: 0.000 | time: 50.95 minutes\n",
      "Epoch: 029/14999 | Loss: 0.0014 | Episodes: 11 | Win count: 29 | Win rate: 0.000 | time: 51.39 minutes\n",
      "Epoch: 030/14999 | Loss: 0.0007 | Episodes: 24 | Win count: 30 | Win rate: 0.000 | time: 52.31 minutes\n",
      "Epoch: 031/14999 | Loss: 0.0010 | Episodes: 9 | Win count: 31 | Win rate: 0.000 | time: 52.66 minutes\n",
      "Epoch: 032/14999 | Loss: 0.0009 | Episodes: 25 | Win count: 32 | Win rate: 0.969 | time: 53.63 minutes\n",
      "Epoch: 033/14999 | Loss: 0.0013 | Episodes: 2 | Win count: 33 | Win rate: 0.969 | time: 53.71 minutes\n",
      "Epoch: 034/14999 | Loss: 0.0010 | Episodes: 18 | Win count: 34 | Win rate: 1.000 | time: 54.41 minutes\n",
      "Epoch: 035/14999 | Loss: 0.0009 | Episodes: 17 | Win count: 35 | Win rate: 1.000 | time: 55.13 minutes\n",
      "Epoch: 036/14999 | Loss: 0.0008 | Episodes: 14 | Win count: 36 | Win rate: 1.000 | time: 55.76 minutes\n",
      "Epoch: 037/14999 | Loss: 0.0005 | Episodes: 9 | Win count: 37 | Win rate: 1.000 | time: 56.18 minutes\n",
      "Epoch: 038/14999 | Loss: 0.0009 | Episodes: 8 | Win count: 38 | Win rate: 1.000 | time: 56.56 minutes\n",
      "Epoch: 039/14999 | Loss: 0.0011 | Episodes: 7 | Win count: 39 | Win rate: 1.000 | time: 56.90 minutes\n",
      "Epoch: 040/14999 | Loss: 0.0003 | Episodes: 99 | Win count: 40 | Win rate: 1.000 | time: 60.82 minutes\n",
      "Epoch: 041/14999 | Loss: 0.0009 | Episodes: 25 | Win count: 41 | Win rate: 1.000 | time: 61.86 minutes\n",
      "Epoch: 042/14999 | Loss: 0.0002 | Episodes: 8 | Win count: 42 | Win rate: 1.000 | time: 62.24 minutes\n",
      "Epoch: 043/14999 | Loss: 0.0005 | Episodes: 40 | Win count: 43 | Win rate: 1.000 | time: 63.86 minutes\n",
      "Epoch: 044/14999 | Loss: 0.0006 | Episodes: 4 | Win count: 44 | Win rate: 1.000 | time: 64.09 minutes\n",
      "Epoch: 045/14999 | Loss: 0.0011 | Episodes: 13 | Win count: 45 | Win rate: 1.000 | time: 64.67 minutes\n",
      "Epoch: 046/14999 | Loss: 0.0005 | Episodes: 42 | Win count: 46 | Win rate: 1.000 | time: 66.37 minutes\n",
      "Epoch: 047/14999 | Loss: 0.0010 | Episodes: 43 | Win count: 47 | Win rate: 1.000 | time: 1.14 hours\n",
      "Epoch: 048/14999 | Loss: 0.0005 | Episodes: 6 | Win count: 48 | Win rate: 1.000 | time: 1.14 hours\n",
      "Epoch: 049/14999 | Loss: 0.0004 | Episodes: 16 | Win count: 49 | Win rate: 1.000 | time: 1.15 hours\n",
      "Epoch: 050/14999 | Loss: 0.0008 | Episodes: 6 | Win count: 50 | Win rate: 1.000 | time: 1.16 hours\n",
      "Epoch: 051/14999 | Loss: 0.0003 | Episodes: 7 | Win count: 51 | Win rate: 1.000 | time: 1.16 hours\n",
      "Epoch: 052/14999 | Loss: 0.0011 | Episodes: 24 | Win count: 52 | Win rate: 1.000 | time: 1.18 hours\n",
      "Epoch: 053/14999 | Loss: 0.0001 | Episodes: 23 | Win count: 53 | Win rate: 1.000 | time: 1.20 hours\n",
      "Epoch: 054/14999 | Loss: 0.0005 | Episodes: 4 | Win count: 54 | Win rate: 1.000 | time: 1.21 hours\n",
      "Epoch: 055/14999 | Loss: 0.0006 | Episodes: 26 | Win count: 55 | Win rate: 1.000 | time: 1.23 hours\n",
      "Epoch: 056/14999 | Loss: 0.0004 | Episodes: 10 | Win count: 56 | Win rate: 1.000 | time: 1.23 hours\n",
      "Epoch: 057/14999 | Loss: 0.0004 | Episodes: 11 | Win count: 57 | Win rate: 1.000 | time: 1.24 hours\n",
      "Epoch: 058/14999 | Loss: 0.0000 | Episodes: 22 | Win count: 58 | Win rate: 1.000 | time: 1.26 hours\n",
      "Epoch: 059/14999 | Loss: 0.0001 | Episodes: 24 | Win count: 59 | Win rate: 1.000 | time: 1.28 hours\n",
      "Epoch: 060/14999 | Loss: 0.0010 | Episodes: 28 | Win count: 60 | Win rate: 1.000 | time: 1.30 hours\n",
      "Epoch: 061/14999 | Loss: 0.0010 | Episodes: 25 | Win count: 61 | Win rate: 1.000 | time: 1.32 hours\n",
      "Epoch: 062/14999 | Loss: 0.0003 | Episodes: 52 | Win count: 62 | Win rate: 1.000 | time: 1.36 hours\n",
      "Epoch: 063/14999 | Loss: 0.0001 | Episodes: 22 | Win count: 63 | Win rate: 1.000 | time: 1.38 hours\n",
      "Epoch: 064/14999 | Loss: 0.0002 | Episodes: 50 | Win count: 64 | Win rate: 1.000 | time: 1.41 hours\n",
      "Epoch: 065/14999 | Loss: 0.0002 | Episodes: 13 | Win count: 65 | Win rate: 1.000 | time: 1.42 hours\n",
      "Epoch: 066/14999 | Loss: 0.0002 | Episodes: 18 | Win count: 66 | Win rate: 1.000 | time: 1.44 hours\n",
      "Epoch: 067/14999 | Loss: 0.0002 | Episodes: 27 | Win count: 67 | Win rate: 1.000 | time: 1.46 hours\n",
      "Epoch: 068/14999 | Loss: 0.0003 | Episodes: 22 | Win count: 68 | Win rate: 1.000 | time: 1.48 hours\n",
      "Epoch: 069/14999 | Loss: 0.0001 | Episodes: 33 | Win count: 69 | Win rate: 1.000 | time: 1.50 hours\n",
      "Epoch: 070/14999 | Loss: 0.0001 | Episodes: 28 | Win count: 70 | Win rate: 1.000 | time: 1.52 hours\n",
      "Epoch: 071/14999 | Loss: 0.0000 | Episodes: 21 | Win count: 71 | Win rate: 1.000 | time: 1.54 hours\n",
      "Epoch: 072/14999 | Loss: 0.0000 | Episodes: 18 | Win count: 72 | Win rate: 1.000 | time: 1.55 hours\n",
      "Epoch: 073/14999 | Loss: 0.0000 | Episodes: 25 | Win count: 73 | Win rate: 1.000 | time: 1.57 hours\n",
      "Epoch: 074/14999 | Loss: 0.0003 | Episodes: 22 | Win count: 74 | Win rate: 1.000 | time: 1.58 hours\n",
      "Epoch: 075/14999 | Loss: 0.0002 | Episodes: 44 | Win count: 75 | Win rate: 1.000 | time: 1.62 hours\n",
      "Epoch: 076/14999 | Loss: 0.0000 | Episodes: 21 | Win count: 76 | Win rate: 1.000 | time: 1.63 hours\n",
      "Epoch: 077/14999 | Loss: 0.0000 | Episodes: 17 | Win count: 77 | Win rate: 1.000 | time: 1.64 hours\n",
      "Epoch: 078/14999 | Loss: 0.0003 | Episodes: 14 | Win count: 78 | Win rate: 1.000 | time: 1.65 hours\n",
      "Epoch: 079/14999 | Loss: 0.0001 | Episodes: 29 | Win count: 79 | Win rate: 1.000 | time: 1.67 hours\n",
      "Epoch: 080/14999 | Loss: 0.0003 | Episodes: 7 | Win count: 80 | Win rate: 1.000 | time: 1.68 hours\n",
      "Epoch: 081/14999 | Loss: 0.0001 | Episodes: 7 | Win count: 81 | Win rate: 1.000 | time: 1.69 hours\n",
      "Epoch: 082/14999 | Loss: 0.0000 | Episodes: 39 | Win count: 82 | Win rate: 1.000 | time: 1.72 hours\n",
      "Epoch: 083/14999 | Loss: 0.0004 | Episodes: 34 | Win count: 83 | Win rate: 1.000 | time: 1.75 hours\n",
      "Epoch: 084/14999 | Loss: 0.0006 | Episodes: 27 | Win count: 84 | Win rate: 1.000 | time: 1.76 hours\n",
      "Epoch: 085/14999 | Loss: 0.0008 | Episodes: 21 | Win count: 85 | Win rate: 1.000 | time: 1.78 hours\n",
      "Epoch: 086/14999 | Loss: 0.0005 | Episodes: 53 | Win count: 86 | Win rate: 1.000 | time: 1.82 hours\n",
      "Epoch: 087/14999 | Loss: 0.0006 | Episodes: 26 | Win count: 87 | Win rate: 1.000 | time: 1.83 hours\n",
      "Epoch: 088/14999 | Loss: 0.0003 | Episodes: 7 | Win count: 88 | Win rate: 1.000 | time: 1.84 hours\n",
      "Epoch: 089/14999 | Loss: 0.0010 | Episodes: 28 | Win count: 89 | Win rate: 1.000 | time: 1.86 hours\n",
      "Epoch: 090/14999 | Loss: 0.0008 | Episodes: 23 | Win count: 90 | Win rate: 1.000 | time: 1.88 hours\n",
      "Epoch: 091/14999 | Loss: 0.0003 | Episodes: 3 | Win count: 91 | Win rate: 1.000 | time: 1.88 hours\n",
      "Epoch: 092/14999 | Loss: 0.0008 | Episodes: 16 | Win count: 92 | Win rate: 1.000 | time: 1.89 hours\n",
      "Epoch: 093/14999 | Loss: 0.0003 | Episodes: 24 | Win count: 93 | Win rate: 1.000 | time: 1.91 hours\n",
      "Epoch: 094/14999 | Loss: 0.0007 | Episodes: 25 | Win count: 94 | Win rate: 1.000 | time: 1.93 hours\n",
      "Epoch: 095/14999 | Loss: 0.0002 | Episodes: 26 | Win count: 95 | Win rate: 1.000 | time: 1.95 hours\n",
      "Epoch: 096/14999 | Loss: 0.0001 | Episodes: 19 | Win count: 96 | Win rate: 1.000 | time: 1.97 hours\n",
      "Epoch: 097/14999 | Loss: 0.0008 | Episodes: 2 | Win count: 97 | Win rate: 1.000 | time: 1.97 hours\n",
      "Epoch: 098/14999 | Loss: 0.0005 | Episodes: 11 | Win count: 98 | Win rate: 1.000 | time: 1.98 hours\n",
      "Epoch: 099/14999 | Loss: 0.0006 | Episodes: 11 | Win count: 99 | Win rate: 1.000 | time: 1.99 hours\n",
      "Epoch: 100/14999 | Loss: 0.0006 | Episodes: 30 | Win count: 100 | Win rate: 1.000 | time: 2.01 hours\n",
      "Epoch: 101/14999 | Loss: 0.0005 | Episodes: 31 | Win count: 101 | Win rate: 1.000 | time: 2.03 hours\n",
      "Epoch: 102/14999 | Loss: 0.0001 | Episodes: 8 | Win count: 102 | Win rate: 1.000 | time: 2.04 hours\n",
      "Epoch: 103/14999 | Loss: 0.0008 | Episodes: 26 | Win count: 103 | Win rate: 1.000 | time: 2.06 hours\n",
      "Epoch: 104/14999 | Loss: 0.0008 | Episodes: 1 | Win count: 104 | Win rate: 1.000 | time: 2.07 hours\n",
      "Epoch: 105/14999 | Loss: 0.0001 | Episodes: 4 | Win count: 105 | Win rate: 1.000 | time: 2.08 hours\n",
      "Epoch: 106/14999 | Loss: 0.0004 | Episodes: 2 | Win count: 106 | Win rate: 1.000 | time: 2.09 hours\n",
      "Epoch: 107/14999 | Loss: 0.0006 | Episodes: 8 | Win count: 107 | Win rate: 1.000 | time: 2.09 hours\n",
      "Epoch: 108/14999 | Loss: 0.0005 | Episodes: 11 | Win count: 108 | Win rate: 1.000 | time: 2.10 hours\n",
      "Epoch: 109/14999 | Loss: 0.0001 | Episodes: 6 | Win count: 109 | Win rate: 1.000 | time: 2.11 hours\n",
      "Epoch: 110/14999 | Loss: 0.0001 | Episodes: 21 | Win count: 110 | Win rate: 1.000 | time: 2.13 hours\n",
      "Epoch: 111/14999 | Loss: 0.0003 | Episodes: 5 | Win count: 111 | Win rate: 1.000 | time: 2.13 hours\n",
      "Epoch: 112/14999 | Loss: 0.0006 | Episodes: 21 | Win count: 112 | Win rate: 1.000 | time: 2.15 hours\n",
      "Epoch: 113/14999 | Loss: 0.0006 | Episodes: 25 | Win count: 113 | Win rate: 1.000 | time: 2.16 hours\n",
      "Epoch: 114/14999 | Loss: 0.0003 | Episodes: 39 | Win count: 114 | Win rate: 1.000 | time: 2.19 hours\n",
      "Epoch: 115/14999 | Loss: 0.0004 | Episodes: 25 | Win count: 115 | Win rate: 1.000 | time: 2.21 hours\n",
      "Epoch: 116/14999 | Loss: 0.0000 | Episodes: 21 | Win count: 116 | Win rate: 1.000 | time: 2.22 hours\n",
      "Epoch: 117/14999 | Loss: 0.0000 | Episodes: 22 | Win count: 117 | Win rate: 1.000 | time: 2.24 hours\n",
      "Epoch: 118/14999 | Loss: 0.0003 | Episodes: 26 | Win count: 118 | Win rate: 1.000 | time: 2.26 hours\n",
      "Epoch: 119/14999 | Loss: 0.0006 | Episodes: 16 | Win count: 119 | Win rate: 1.000 | time: 2.27 hours\n",
      "Epoch: 120/14999 | Loss: 0.0001 | Episodes: 19 | Win count: 120 | Win rate: 1.000 | time: 2.28 hours\n",
      "Epoch: 121/14999 | Loss: 0.0000 | Episodes: 12 | Win count: 121 | Win rate: 1.000 | time: 2.29 hours\n",
      "Epoch: 122/14999 | Loss: 0.0000 | Episodes: 23 | Win count: 122 | Win rate: 1.000 | time: 2.31 hours\n",
      "Epoch: 123/14999 | Loss: 0.0000 | Episodes: 21 | Win count: 123 | Win rate: 1.000 | time: 2.32 hours\n",
      "Epoch: 124/14999 | Loss: 0.0005 | Episodes: 26 | Win count: 124 | Win rate: 1.000 | time: 2.34 hours\n",
      "Epoch: 125/14999 | Loss: 0.0003 | Episodes: 12 | Win count: 125 | Win rate: 1.000 | time: 2.35 hours\n",
      "Epoch: 126/14999 | Loss: 0.0000 | Episodes: 28 | Win count: 126 | Win rate: 1.000 | time: 2.37 hours\n",
      "Reached 100% win rate at epoch: 126\n",
      "n_epoch: 126, max_mem: 512, data: 32, time: 2.38 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8566.279499"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model and train network\n",
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will check to see if the model passes the completion check. Note: This could take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29d3f889d00>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if modelpasses the completion check\n",
    "completion_check(model, qmaze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29d3ff670a0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test model\n",
    "pirate_start = (0, 0)\n",
    "play_game(model, qmaze, pirate_start)\n",
    "show(qmaze)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
